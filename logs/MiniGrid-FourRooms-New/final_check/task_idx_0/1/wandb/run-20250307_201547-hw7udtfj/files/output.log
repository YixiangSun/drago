
[34m[1mLogs will be synced with wandb.
 [34mtrain[39m   [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m 0.0              [30mT:[39m 0:00:14
 [32meval[39m    [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m 0.7              [30mT:[39m 0:00:14
 [34mtrain[39m   [30mE:[39m 2                [30mS:[39m 200              [30mR:[39m 0.0              [30mT:[39m 0:00:40
 [34mtrain[39m   [30mE:[39m 3                [30mS:[39m 400              [30mR:[39m 0.0              [30mT:[39m 0:00:54
 [34mtrain[39m   [30mE:[39m 4                [30mS:[39m 600              [30mR:[39m 0.0              [30mT:[39m 0:01:09
 [34mtrain[39m   [30mE:[39m 5                [30mS:[39m 800              [30mR:[39m 0.0              [30mT:[39m 0:01:24
 [34mtrain[39m   [30mE:[39m 6                [30mS:[39m 1,000            [30mR:[39m 0.0              [30mT:[39m 0:01:38
 [34mtrain[39m   [30mE:[39m 7                [30mS:[39m 1,200            [30mR:[39m 0.0              [30mT:[39m 0:02:58
 [34mtrain[39m   [30mE:[39m 8                [30mS:[39m 1,400            [30mR:[39m 0.0              [30mT:[39m 0:04:15
Traceback (most recent call last):
  File "/oscar/data/gdk/ysun133/drago/src/train.py", line 204, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "/oscar/data/gdk/ysun133/drago/src/train.py", line 134, in train
    action = agent.reviewer.plan(reviewer_buffer, pseudo_counts,\
  File "/users/ysun133/miniconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/oscar/data/gdk/ysun133/drago/src/algorithm/drago.py", line 211, in plan
    value = self.estimate_value(z, actions, horizon, pseudo_counts, eval_mode=eval_mode, key=key).nan_to_num_(0)
  File "/users/ysun133/miniconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/oscar/data/gdk/ysun133/drago/src/algorithm/drago.py", line 106, in estimate_value
    discounted_reward = reward * torch.tensor(pseudo_counts.get_intrinsic_rewards(z))
  File "/oscar/data/gdk/ysun133/drago/src/algorithm/helper.py", line 528, in get_intrinsic_rewards
    intrinsic_reward = 1 / torch.sqrt((states_expanded * mask).sum(dim=(1, 2, 3)) + 0.99999) - 1 / 99999
KeyboardInterrupt
Traceback (most recent call last):
  File "/oscar/data/gdk/ysun133/drago/src/train.py", line 204, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "/oscar/data/gdk/ysun133/drago/src/train.py", line 134, in train
    action = agent.reviewer.plan(reviewer_buffer, pseudo_counts,\
  File "/users/ysun133/miniconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/oscar/data/gdk/ysun133/drago/src/algorithm/drago.py", line 211, in plan
    value = self.estimate_value(z, actions, horizon, pseudo_counts, eval_mode=eval_mode, key=key).nan_to_num_(0)
  File "/users/ysun133/miniconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/oscar/data/gdk/ysun133/drago/src/algorithm/drago.py", line 106, in estimate_value
    discounted_reward = reward * torch.tensor(pseudo_counts.get_intrinsic_rewards(z))
  File "/oscar/data/gdk/ysun133/drago/src/algorithm/helper.py", line 528, in get_intrinsic_rewards
    intrinsic_reward = 1 / torch.sqrt((states_expanded * mask).sum(dim=(1, 2, 3)) + 0.99999) - 1 / 99999
KeyboardInterrupt